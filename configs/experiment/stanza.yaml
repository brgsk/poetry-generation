# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

defaults:
  - override /mode: exp
  - override /trainer: null
  - override /model: null
  - override /datamodule: null
  - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "stanza"

seed: 12345

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  min_epochs: 1
  max_epochs: 10
  gradient_clip_val: 0.5
  accumulate_grad_batches: 8
  weights_summary: "full"
  num_sanity_val_steps: 0

model:
  _target_: src.models.poet_lightning.PoetLightningModel
  lr: 0.0001
  warmup_steps: 0
  vocab_size: ???
  n_positions: 512
  pretrained_name_or_path: "dkleczek/papuGaPT2"

datamodule:
  _target_: src.datamodules.stanza_datamodule.StanzaDataModule

  res_dir: ${res_dir} # data_dir is specified in config.yaml
  batch_size: 2
  train_val_ratio: 0.8
  num_workers: 16
  pin_memory: True

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/loss"
    mode: "min"
    save_top_k: 1
    save_last: True
    verbose: False
    dirpath: "checkpoints/"
    filename: "best-checkpoint-{epoch:02d}-val_loss-{val/loss:.2f}"
    auto_insert_metric_name: False
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/loss"
    mode: "min"
    patience: 2
    min_delta: 0.0
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
